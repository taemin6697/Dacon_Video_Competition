{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3999dc49",
   "metadata": {},
   "source": [
    "## 라이브러리 IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58fba997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#import albumentations as A\n",
    "#from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision.models as models\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "570dbad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'FPS':30,\n",
    "    'IMG_SIZE':32,\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    'BATCH_SIZE':4,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19956db",
   "metadata": {},
   "source": [
    "# 모델 생성 Facebook의 Timesformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2a32d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-ssv2 and are newly initialized because the shapes did not match:\n",
      "- timesformer.embeddings.time_embeddings: found shape torch.Size([1, 8, 768]) in the checkpoint and torch.Size([1, 30, 768]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([174, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([174]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\n",
    "from transformers import AutoImageProcessor, TimesformerForVideoClassification\n",
    "\n",
    "model = TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-ssv2\",num_labels=5,num_frames=30,ignore_mismatched_sizes=True)\n",
    "feature_extractor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-ssv2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9d6e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37e7aa6",
   "metadata": {},
   "source": [
    "## train.csv 파일을 읽어온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e951d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "661980d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_000</td>\n",
       "      <td>./train/TRAIN_000.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_001</td>\n",
       "      <td>./train/TRAIN_001.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_002</td>\n",
       "      <td>./train/TRAIN_002.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_003</td>\n",
       "      <td>./train/TRAIN_003.mp4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_004</td>\n",
       "      <td>./train/TRAIN_004.mp4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>TRAIN_605</td>\n",
       "      <td>./train/TRAIN_605.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>TRAIN_606</td>\n",
       "      <td>./train/TRAIN_606.mp4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>TRAIN_607</td>\n",
       "      <td>./train/TRAIN_607.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>TRAIN_608</td>\n",
       "      <td>./train/TRAIN_608.mp4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>TRAIN_609</td>\n",
       "      <td>./train/TRAIN_609.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>610 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                   path  label\n",
       "0    TRAIN_000  ./train/TRAIN_000.mp4      3\n",
       "1    TRAIN_001  ./train/TRAIN_001.mp4      0\n",
       "2    TRAIN_002  ./train/TRAIN_002.mp4      1\n",
       "3    TRAIN_003  ./train/TRAIN_003.mp4      4\n",
       "4    TRAIN_004  ./train/TRAIN_004.mp4      4\n",
       "..         ...                    ...    ...\n",
       "605  TRAIN_605  ./train/TRAIN_605.mp4      0\n",
       "606  TRAIN_606  ./train/TRAIN_606.mp4      2\n",
       "607  TRAIN_607  ./train/TRAIN_607.mp4      1\n",
       "608  TRAIN_608  ./train/TRAIN_608.mp4      4\n",
       "609  TRAIN_609  ./train/TRAIN_609.mp4      0\n",
       "\n",
       "[610 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "l = df['label']\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "# fit_transform은 train에만 사용하고 test에는 학습된 인코더에 fit만 해야한다\n",
    "train_cat = ohe.fit_transform(df[['label']])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c4dfa",
   "metadata": {},
   "source": [
    "## VAL과 TRAIN 데이터셋 0.1 사이즈로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec92e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, _, _ = train_test_split(df, df['label'], test_size=0.1, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e930db",
   "metadata": {},
   "source": [
    "# Custom 데이터셋 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8599259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, video_path_list, label_list):\n",
    "        self.video_path_list = video_path_list\n",
    "        self.label_list = label_list\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        frames = list(np.array((self.get_video(self.video_path_list[index]))))\n",
    "        #print(frames)\n",
    "        #video = list(np.random.randn(16, 3, 224, 224))\n",
    "        #print('gsdfsdfdsafdsafadf')\n",
    "        #print(video)\n",
    "        pixel_values = feature_extractor(frames,return_tensor=\"pt\").pixel_values\n",
    "        if self.label_list is not None:\n",
    "            cell = [0] *5\n",
    "            label = self.label_list[index]\n",
    "            cell[label]+=1\n",
    "            label = np.array(cell)\n",
    "            label = label.astype(np.float32).tolist()\n",
    "            #print(label)\n",
    "            encoding = {\"pixel_values\" : torch.tensor(pixel_values).squeeze(),'labels': torch.tensor(label).squeeze()} \n",
    "            return encoding\n",
    "        else:\n",
    "            encoding = {\"pixel_values\" : torch.tensor(pixel_values).squeeze()}\n",
    "            return encoding\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_path_list)\n",
    "    \n",
    "    def get_video(self, path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        for _ in range(CFG['FPS']):\n",
    "            _, img = cap.read()\n",
    "            #img = cv2.resize(img, (224, 224))\n",
    "            #img = img / 255.\n",
    "            frames.append(img)\n",
    "        frames = np.array(tuple(frames))\n",
    "        return torch.FloatTensor(np.array(frames)).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ad78e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train['path'].values, train['label'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val['path'].values, val['label'].values)\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f7746",
   "metadata": {},
   "source": [
    "# 훈련에 필요한 각종 파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1549e844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "batch_size = 2\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = 'times',\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=10,\n",
    "    save_total_limit = 10,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps=275\n",
    "    #max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18b07b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5e03f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53c964d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions.\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd693e3f",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01f2155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    #compute_metrics=compute_metrics,\n",
    "    #data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d640baa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 549\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2750\n",
      "  Number of trainable parameters = 121279493\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='2750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/2750 00:01 < 50:18, 0.91 it/s, Epoch 0.01/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34576\\4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Times\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1545\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1546\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1547\u001b[1;33m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1548\u001b[0m         )\n\u001b[0;32m   1549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Times\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1764\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1765\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Times\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Times\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Times\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Times\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34576\\2905290790.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#print('gsdfsdfdsafdsafadf')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#print(video)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mpixel_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_list\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Times\\lib\\site-packages\\transformers\\image_processing_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;34m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Times\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(self, videos, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             ]\n\u001b[1;32m--> 379\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mvideo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvideos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m         ]\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Times\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    377\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             ]\n\u001b[1;32m--> 379\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mvideo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvideos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m         ]\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Times\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    375\u001b[0m                     \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m                 )\n\u001b[1;32m--> 377\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             ]\n\u001b[0;32m    379\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mvideo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvideos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Times\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py\u001b[0m in \u001b[0;36m_preprocess_image\u001b[1;34m(self, image, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, data_format)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdo_center_crop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Times\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, image, size, resample, data_format, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Size must have 'height' and 'width' or 'shortest_edge' as keys. Got {size.keys()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     def center_crop(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Times\\lib\\site-packages\\transformers\\image_transforms.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(image, size, resample, reducing_gap, data_format, return_numpy)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[1;31m# PIL images are in the format (width, height)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m     \u001b[0mresized_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreducing_gap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreducing_gap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_numpy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Times\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2190\u001b[0m                 )\n\u001b[0;32m   2191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2192\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b394d7e",
   "metadata": {},
   "source": [
    "# 훈련했던 모델의 가중치 파일과 전처리기를 불러옵니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b4068",
   "metadata": {},
   "source": [
    "## 처음부터 가중치 파일로부터 추론할경우 아래의 model = TimesformerForVideoClassification.from_pretrained(\"E:/timesformer-base-ssv2/checkpoint-2750\")에서 E:/timesformer-base-ssv2/checkpoint-2750를 경로에 맞게 바꾸시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eef50a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./times/checkpoint-2750\\config.json\n",
      "Model config TimesformerConfig {\n",
      "  \"_name_or_path\": \"facebook/timesformer-base-finetuned-ssv2\",\n",
      "  \"architectures\": [\n",
      "    \"TimesformerForVideoClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"attention_type\": \"divided_space_time\",\n",
      "  \"drop_path_rate\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-06,\n",
      "  \"model_type\": \"timesformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_frames\": 30,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "loading weights file ./times/checkpoint-2750\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing TimesformerForVideoClassification.\n",
      "\n",
      "All the weights of TimesformerForVideoClassification were initialized from the model checkpoint at ./times/checkpoint-2750.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TimesformerForVideoClassification for predictions without further training.\n",
      "loading configuration file preprocessor_config.json from cache at C:\\Users\\tm011/.cache\\huggingface\\hub\\models--facebook--timesformer-base-finetuned-ssv2\\snapshots\\3b045270472c79cf9c1b60189ba425e92ed7f004\\preprocessor_config.json\n",
      "Image processor VideoMAEImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.45,\n",
      "    0.45,\n",
      "    0.45\n",
      "  ],\n",
      "  \"image_processor_type\": \"VideoMAEImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.225,\n",
      "    0.225,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 224\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = TimesformerForVideoClassification.from_pretrained(\"./times/checkpoint-2750\")\n",
    "feature_extractor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-ssv2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a777d1e",
   "metadata": {},
   "source": [
    "# TESE.csv파일을 읽어와 추론을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6a52d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19421566",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test['path'].values, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc963158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c179bad39b4f92a292a4a6e8afe327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.4457,  4.6761, -6.3286, -5.2873, -4.9104]], device='cuda:0')\n",
      "tensor([[-5.9229, -5.9610, -3.7331,  3.7620, -6.7892]], device='cuda:0')\n",
      "tensor([[ 5.6294, -5.6800, -5.6347, -5.0933, -5.4685]], device='cuda:0')\n",
      "tensor([[-5.9834, -5.1947,  4.6965, -4.7221, -6.8901]], device='cuda:0')\n",
      "tensor([[-5.1755, -6.0655, -4.6566, -5.7257,  5.6133]], device='cuda:0')\n",
      "tensor([[-5.1996, -5.6043,  5.7051, -5.9783, -4.9697]], device='cuda:0')\n",
      "tensor([[-5.1315, -5.3775, -5.9836, -5.7376,  5.4706]], device='cuda:0')\n",
      "tensor([[-6.2472, -6.2029, -4.8264,  4.4311, -5.5311]], device='cuda:0')\n",
      "tensor([[-3.8584,  2.6701, -6.5978, -5.7971, -5.2993]], device='cuda:0')\n",
      "tensor([[-6.2826, -6.4962,  0.1561, -2.1507, -5.1657]], device='cuda:0')\n",
      "tensor([[-5.6307, -6.0634, -3.4440,  0.5440, -6.8746]], device='cuda:0')\n",
      "tensor([[-6.2053, -6.0817, -2.0660,  1.8065, -6.7433]], device='cuda:0')\n",
      "tensor([[ 4.9322, -5.3245, -5.6257, -5.7592, -5.3636]], device='cuda:0')\n",
      "tensor([[-6.5837, -5.6309, -5.0798, -5.7058,  4.9977]], device='cuda:0')\n",
      "tensor([[-6.2009,  5.3566, -4.8344, -5.9745, -5.5765]], device='cuda:0')\n",
      "tensor([[-6.3705, -4.2832, -5.6136, -6.5364,  3.4068]], device='cuda:0')\n",
      "tensor([[-5.4077, -6.6953, -2.3717,  3.3073, -7.1060]], device='cuda:0')\n",
      "tensor([[-4.5826,  4.7724, -5.3397, -6.0225, -5.3733]], device='cuda:0')\n",
      "tensor([[-5.4981, -4.0954, -6.4670, -5.9592,  3.8278]], device='cuda:0')\n",
      "tensor([[-6.1716,  5.0000, -5.0362, -5.7982, -5.7231]], device='cuda:0')\n",
      "tensor([[-6.3799, -5.9150,  3.5226, -3.2256, -5.8401]], device='cuda:0')\n",
      "tensor([[-5.8084, -5.7602,  3.2519, -5.2131, -5.6277]], device='cuda:0')\n",
      "tensor([[-6.5641, -7.1398, -2.2396,  0.4100, -4.6649]], device='cuda:0')\n",
      "tensor([[-5.1162, -5.4863, -6.2615,  5.3526, -5.4896]], device='cuda:0')\n",
      "tensor([[-5.4899, -5.5830,  5.2305, -4.9974, -5.2387]], device='cuda:0')\n",
      "tensor([[-5.9672, -5.5644, -5.6676,  5.1388, -4.9924]], device='cuda:0')\n",
      "tensor([[-5.1270,  4.7194, -7.0790, -4.8526, -5.0304]], device='cuda:0')\n",
      "tensor([[-5.8511, -5.0371, -6.2771, -5.2519,  5.3552]], device='cuda:0')\n",
      "tensor([[-5.2423, -5.6064, -6.1777, -5.1917,  5.4654]], device='cuda:0')\n",
      "tensor([[-4.8990,  4.4284, -6.3951, -5.1673, -5.5897]], device='cuda:0')\n",
      "tensor([[-5.9706, -7.4475, -3.3873,  4.0281, -5.8551]], device='cuda:0')\n",
      "tensor([[-4.8288,  5.6860, -5.1634, -5.1308, -5.6516]], device='cuda:0')\n",
      "tensor([[ 3.7049, -4.5453, -6.3871, -5.0924, -5.8874]], device='cuda:0')\n",
      "tensor([[-5.4149, -5.7485, -5.6325, -5.2408,  5.6055]], device='cuda:0')\n",
      "tensor([[-5.0305, -5.4277, -6.2057,  5.9748, -4.3584]], device='cuda:0')\n",
      "tensor([[-5.3377, -4.2873, -5.2851, -6.6213,  4.8396]], device='cuda:0')\n",
      "tensor([[-4.6249, -4.6637,  4.2570, -6.1158, -6.5550]], device='cuda:0')\n",
      "tensor([[-5.4197, -6.3161, -5.4346,  4.8732, -5.4708]], device='cuda:0')\n",
      "tensor([[-5.9746, -6.7202,  3.1859, -3.0787, -6.5950]], device='cuda:0')\n",
      "tensor([[ 4.7404, -4.8832, -5.7411, -6.6450, -5.2473]], device='cuda:0')\n",
      "tensor([[-5.5032, -5.7914,  4.4766, -5.4987, -5.5573]], device='cuda:0')\n",
      "tensor([[-5.7797, -5.2202, -5.4565, -3.3022,  4.4980]], device='cuda:0')\n",
      "tensor([[-4.9262, -6.0638, -4.8084, -6.1241,  5.0443]], device='cuda:0')\n",
      "tensor([[-5.9481, -6.1794, -5.5331,  5.3166, -5.4664]], device='cuda:0')\n",
      "tensor([[-5.8560, -6.0250, -4.4584,  4.1776, -5.8518]], device='cuda:0')\n",
      "tensor([[-5.3185, -5.9766, -5.2365, -6.0488,  5.1491]], device='cuda:0')\n",
      "tensor([[-5.4808, -4.9156, -6.2582, -5.3947,  4.7731]], device='cuda:0')\n",
      "tensor([[ 3.5260, -6.8625, -4.6738, -5.3347, -4.5100]], device='cuda:0')\n",
      "tensor([[-5.5840, -5.4354, -5.1326, -5.3676,  5.9634]], device='cuda:0')\n",
      "tensor([[-5.7183,  4.3052, -5.9628, -6.1263, -5.3218]], device='cuda:0')\n",
      "tensor([[-4.7642, -4.8810, -5.2721,  5.3322, -5.4923]], device='cuda:0')\n",
      "tensor([[-5.0742, -5.7271, -6.2907, -5.5694,  5.1909]], device='cuda:0')\n",
      "tensor([[-5.8740, -4.7263, -5.4027, -6.4809,  4.5861]], device='cuda:0')\n",
      "tensor([[ 5.2172, -4.8039, -5.7013, -5.8163, -5.6831]], device='cuda:0')\n",
      "tensor([[-5.5370,  4.1681, -2.9795, -6.7275, -6.6339]], device='cuda:0')\n",
      "tensor([[-5.8614, -5.9404, -4.6607,  4.2536, -5.1494]], device='cuda:0')\n",
      "tensor([[-6.4421, -5.0165, -5.2419, -3.8307,  4.2988]], device='cuda:0')\n",
      "tensor([[-5.7896, -6.2136,  4.3171, -4.8692, -5.5337]], device='cuda:0')\n",
      "tensor([[ 4.9892, -5.8110, -6.1439, -5.3411, -4.9693]], device='cuda:0')\n",
      "tensor([[-6.3600, -6.1723, -6.3144, -4.3156,  4.5140]], device='cuda:0')\n",
      "tensor([[ 4.6661, -5.9499, -5.3775, -4.5769, -5.4985]], device='cuda:0')\n",
      "tensor([[-5.5284, -5.0104, -5.2493,  5.7707, -5.1280]], device='cuda:0')\n",
      "tensor([[-5.6680, -5.8285, -5.7842, -4.9274,  4.4059]], device='cuda:0')\n",
      "tensor([[-5.6401, -5.0745,  3.6591, -4.9623, -6.1945]], device='cuda:0')\n",
      "tensor([[-5.9263, -5.2079, -6.2339, -5.2408,  5.1309]], device='cuda:0')\n",
      "tensor([[-4.1608, -6.5067,  2.5018, -4.4350, -6.9472]], device='cuda:0')\n",
      "tensor([[-4.9111,  5.0080, -5.5917, -5.6829, -4.9450]], device='cuda:0')\n",
      "tensor([[ 5.2214, -6.0691, -5.2952, -4.3781, -4.9544]], device='cuda:0')\n",
      "tensor([[-6.0446, -6.2501,  2.0196, -3.6970, -7.1979]], device='cuda:0')\n",
      "tensor([[-5.5964, -5.4866, -5.2582, -5.7171,  5.7988]], device='cuda:0')\n",
      "tensor([[-5.9366, -5.7067, -5.1500,  5.4816, -5.3581]], device='cuda:0')\n",
      "tensor([[-4.8772,  4.1816, -4.5852, -6.5481, -6.1345]], device='cuda:0')\n",
      "tensor([[-6.1489, -6.2508, -2.0286,  2.0594, -6.9971]], device='cuda:0')\n",
      "tensor([[-4.8647, -4.8788, -6.0648,  3.4776, -6.1650]], device='cuda:0')\n",
      "tensor([[-5.7136, -5.2292, -6.0293,  5.6005, -5.2339]], device='cuda:0')\n",
      "tensor([[ 5.3513, -5.4387, -5.8377, -5.5352, -5.3241]], device='cuda:0')\n",
      "tensor([[-4.4997, -6.0238, -5.3115, -5.9318,  4.0981]], device='cuda:0')\n",
      "tensor([[-6.7636, -4.6323, -4.6621, -5.8473,  5.3715]], device='cuda:0')\n",
      "tensor([[-5.1770, -6.1839, -4.9446,  4.8271, -5.0188]], device='cuda:0')\n",
      "tensor([[ 1.9561, -5.7442, -4.7704, -5.0106, -7.0467]], device='cuda:0')\n",
      "tensor([[-4.5779,  2.4450, -6.2722, -6.2469, -6.6447]], device='cuda:0')\n",
      "tensor([[-5.3587, -4.7353, -7.1529, -4.3587,  4.5089]], device='cuda:0')\n",
      "tensor([[-5.7233, -5.2366, -4.6040,  4.7598, -4.8304]], device='cuda:0')\n",
      "tensor([[-4.3798,  4.6564, -5.7493, -5.9818, -6.3979]], device='cuda:0')\n",
      "tensor([[-5.3822, -5.8829,  0.0248, -1.0606, -6.7350]], device='cuda:0')\n",
      "tensor([[-5.5843, -4.1028,  4.7405, -5.3963, -6.3775]], device='cuda:0')\n",
      "tensor([[-6.5411, -5.4223, -4.9288, -5.4849,  3.8874]], device='cuda:0')\n",
      "tensor([[-5.0565, -5.2770, -5.7813,  4.8847, -5.8306]], device='cuda:0')\n",
      "tensor([[ 4.8760, -5.8684, -4.4933, -5.6007, -5.8259]], device='cuda:0')\n",
      "tensor([[-6.3792, -6.4559,  2.8324, -3.1222, -6.0648]], device='cuda:0')\n",
      "tensor([[-4.6675,  5.1665, -6.6374, -5.2456, -5.7518]], device='cuda:0')\n",
      "tensor([[-5.3410, -7.0262,  3.2737, -5.5921, -4.3736]], device='cuda:0')\n",
      "tensor([[-6.4305, -6.1730,  0.7002,  0.3399, -7.4178]], device='cuda:0')\n",
      "tensor([[ 5.0903, -5.2648, -5.7339, -5.5176, -5.6312]], device='cuda:0')\n",
      "tensor([[-5.4444, -5.3563, -4.8240, -4.8636,  5.4022]], device='cuda:0')\n",
      "tensor([[-5.0953,  5.1423, -5.9130, -5.4846, -5.1538]], device='cuda:0')\n",
      "tensor([[-5.4891, -5.3624, -6.0980, -5.4600,  5.1101]], device='cuda:0')\n",
      "tensor([[ 4.8660, -4.7200, -6.1831, -6.0143, -5.7277]], device='cuda:0')\n",
      "tensor([[-6.2734, -7.1592,  0.8709, -4.0054, -4.7211]], device='cuda:0')\n",
      "tensor([[-5.6744, -5.5846, -5.9580,  5.6003, -4.9931]], device='cuda:0')\n",
      "tensor([[-4.9221, -4.6439,  5.9579, -6.4976, -5.8424]], device='cuda:0')\n",
      "tensor([[-6.6145, -5.3865, -6.3451, -3.8997,  3.2352]], device='cuda:0')\n",
      "tensor([[-5.7598,  5.4221, -5.2008, -5.0735, -5.7391]], device='cuda:0')\n",
      "tensor([[-5.5864, -5.0604, -6.5764, -4.9838,  4.9921]], device='cuda:0')\n",
      "tensor([[ 4.2953, -6.5345, -4.1453, -5.3246, -6.2080]], device='cuda:0')\n",
      "tensor([[-7.4708, -8.1880,  0.2390, -0.4930, -2.9683]], device='cuda:0')\n",
      "tensor([[ 5.3796, -4.2354, -5.9985, -5.7235, -6.1496]], device='cuda:0')\n",
      "tensor([[ 4.2956, -5.1377, -5.6683, -5.2973, -6.2118]], device='cuda:0')\n",
      "tensor([[-5.4993, -6.8153, -4.5271,  4.1970, -5.3655]], device='cuda:0')\n",
      "tensor([[-5.7461,  5.4643, -4.7693, -6.1972, -5.4130]], device='cuda:0')\n",
      "tensor([[ 4.1153, -5.0984, -5.1358, -5.6844, -5.9274]], device='cuda:0')\n",
      "tensor([[-5.0467,  4.6243, -5.9743, -5.8924, -5.9792]], device='cuda:0')\n",
      "tensor([[-4.3082, -5.6755, -6.1373, -5.4689,  5.5180]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.3840, -5.4612, -4.8333,  4.6005, -6.2425]], device='cuda:0')\n",
      "tensor([[-5.8950,  4.8671, -5.3056, -4.6664, -6.3811]], device='cuda:0')\n",
      "tensor([[-5.3992,  4.4882, -6.1350, -5.5738, -5.5669]], device='cuda:0')\n",
      "tensor([[-4.5596, -3.8008,  1.2820, -4.4346, -8.0052]], device='cuda:0')\n",
      "tensor([[-5.6932,  5.3402, -5.4417, -5.5394, -6.0742]], device='cuda:0')\n",
      "tensor([[-5.3977,  6.1791, -5.2133, -5.7320, -5.4290]], device='cuda:0')\n",
      "tensor([[-5.4625, -5.7001,  1.9833, -3.6620, -7.3164]], device='cuda:0')\n",
      "tensor([[-5.8376, -4.3633, -4.0758, -4.9941, -3.3360]], device='cuda:0')\n",
      "tensor([[-6.2549, -6.0618, -4.3074,  4.2416, -5.7237]], device='cuda:0')\n",
      "tensor([[-4.9596,  4.5359, -5.9547, -4.9967, -5.1517]], device='cuda:0')\n",
      "tensor([[-7.0433,  4.7003, -4.5260, -4.9491, -5.7864]], device='cuda:0')\n",
      "tensor([[-6.0319, -6.3961, -3.9849,  3.1077, -6.4624]], device='cuda:0')\n",
      "tensor([[-5.6153, -5.6684, -4.9411, -5.7821,  5.7118]], device='cuda:0')\n",
      "tensor([[ 5.5260, -5.0707, -5.0350, -5.7433, -5.9436]], device='cuda:0')\n",
      "tensor([[-5.6150, -6.0794,  5.0756, -4.3033, -6.1892]], device='cuda:0')\n",
      "tensor([[-5.5064,  4.7498, -5.7374, -5.4466, -5.6162]], device='cuda:0')\n",
      "tensor([[-5.8243, -5.8761, -5.3522, -6.0051,  4.4655]], device='cuda:0')\n",
      "tensor([[ 4.9173, -5.1442, -5.9340, -5.5465, -5.5809]], device='cuda:0')\n",
      "tensor([[ 0.4959, -2.8646, -6.0191, -6.6046, -6.5091]], device='cuda:0')\n",
      "tensor([[-5.7580, -5.4164, -5.8349, -5.5946,  5.7638]], device='cuda:0')\n",
      "tensor([[-6.9507, -5.7728,  0.3299, -1.4234, -6.0481]], device='cuda:0')\n",
      "tensor([[-5.9287, -6.0610, -5.9102, -5.9160,  3.5632]], device='cuda:0')\n",
      "tensor([[-5.0156, -5.1039, -5.4761, -5.7968,  5.9576]], device='cuda:0')\n",
      "tensor([[ 5.0207, -4.8396, -6.5789, -5.6253, -4.7732]], device='cuda:0')\n",
      "tensor([[-6.8002, -6.3509,  3.9711, -4.8206, -4.9516]], device='cuda:0')\n",
      "tensor([[-5.7052, -4.0354, -1.8356,  1.7605, -7.3977]], device='cuda:0')\n",
      "tensor([[ 3.3395, -4.5067, -5.5792, -6.7807, -6.1118]], device='cuda:0')\n",
      "tensor([[ 4.8475, -4.2815, -6.2782, -5.8073, -6.0108]], device='cuda:0')\n",
      "tensor([[ 4.9618, -5.7821, -4.9865, -5.4307, -5.3793]], device='cuda:0')\n",
      "tensor([[-5.2364, -5.9215, -5.2679, -5.5315,  5.7128]], device='cuda:0')\n",
      "tensor([[-5.5218,  4.0899, -4.4639, -7.6074, -4.7164]], device='cuda:0')\n",
      "tensor([[-5.7290, -5.1642, -5.6300,  5.7532, -5.3612]], device='cuda:0')\n",
      "tensor([[-4.1837,  4.2974, -5.9328, -5.0730, -6.5467]], device='cuda:0')\n",
      "tensor([[-6.7092, -7.2733,  2.4905, -1.8451, -6.1967]], device='cuda:0')\n",
      "tensor([[-6.1275, -5.4652,  4.5348, -3.5240, -6.0237]], device='cuda:0')\n",
      "tensor([[-4.9443, -5.6864,  4.4845, -4.8175, -6.1137]], device='cuda:0')\n",
      "tensor([[-5.9590, -4.3540, -5.9949, -6.8345,  4.4076]], device='cuda:0')\n",
      "tensor([[ 5.5896, -5.8026, -4.7796, -5.0805, -5.6525]], device='cuda:0')\n",
      "tensor([[-7.4422,  1.5023, -5.1891, -5.0930, -5.6411]], device='cuda:0')\n",
      "tensor([[-5.0986, -5.1406,  3.8964, -4.4163, -7.0650]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.eval()\n",
    "model.to(device)\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_id, x in enumerate(tqdm(test_loader)):\n",
    "        outputs = model(x['pixel_values'].to(device))\n",
    "        logits = outputs.logits\n",
    "        print(logits)\n",
    "        probs  = logits.cpu().detach().numpy()\n",
    "        class_ = np.argmax(probs)\n",
    "        predictions.append(class_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39c74251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f7a1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eed19049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  label\n",
       "0  TEST_000      1\n",
       "1  TEST_001      3\n",
       "2  TEST_002      0\n",
       "3  TEST_003      2\n",
       "4  TEST_004      4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['label'] = predictions\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0156a39",
   "metadata": {},
   "source": [
    "## 최종 csv파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06ab34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./timesformer_submit_video-re.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823189e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Times",
   "language": "python",
   "name": "times"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
